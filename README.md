# Stock ETL & Analysis Project
## ğŸ“Œ Project Overview

This project extracts, transforms, and analyzes historical stock data for multiple companies. It includes:

ETL pipeline: Fetch data from Yahoo Finance, clean it, and load into PostgreSQL.

Analysis scripts: Perform multiple financial analyses and generate charts and tables.

Outputs: All results are saved in the outputs/ folder, organized by analysis type.


## ğŸ—‚ Folder Structure

```text
stock-etl/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Raw downloaded stock CSVs
â”‚   â””â”€â”€ cleaned/          # Cleaned CSVs ready for DB
â”œâ”€â”€ outputs/              # Analysis results (images & CSVs)
â”‚   â”œâ”€â”€ analyze_data/
â”‚   â”œâ”€â”€ correlation_volatility/
â”‚   â”œâ”€â”€ cumulative_returns/
â”‚   â”œâ”€â”€ drawdowns/
â”‚   â”œâ”€â”€ moving_average_crossovers/
â”‚   â”œâ”€â”€ return_vs_volatility/
â”‚   â”œâ”€â”€ sector_daily_returns/
â”‚   â”œâ”€â”€ sector_trends/
â”‚   â”œâ”€â”€ top_movers_sector_summary/
â”‚   â”œâ”€â”€ trading_volume_analysis/
â”‚   â””â”€â”€ volatility_vs_return/
â”œâ”€â”€ venv/                 # Python virtual environment
â”œâ”€â”€ analyze_data.py
â”œâ”€â”€ check_cleaned.py
â”œâ”€â”€ clean_data.py
â”œâ”€â”€ correlation_volatility.py
â”œâ”€â”€ cumulative_returns.py
â”œâ”€â”€ db_connection.py
â”œâ”€â”€ drawdowns.py
â”œâ”€â”€ export_summary_tables.py
â”œâ”€â”€ fetch_data.py
â”œâ”€â”€ load_to_db.py
â”œâ”€â”€ moving_average_crossovers.py
â”œâ”€â”€ return_vs_volatility.py
â”œâ”€â”€ seasonality_analysis.py
â”œâ”€â”€ sector_daily_returns.py
â”œâ”€â”€ sector_trends.py
â”œâ”€â”€ test_db_connection.py
â”œâ”€â”€ top_movers_sector_summary.py
â”œâ”€â”€ trading_volume_analysis.py
â””â”€â”€ volatility_vs_return.py
```

## ğŸ›  Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone <your-github-repo-url>
cd stock-etl
```
### 2ï¸âƒ£ Create & Activate Virtual Environment
```bash
python -m venv venv
Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```

### 3ï¸âƒ£ Install Required Packages

```bash
pip install -r requirements.txt


Typical packages included: pandas, numpy, matplotlib, seaborn, sqlalchemy, psycopg2-binary, yfinance.
```

### 4ï¸âƒ£ Setup PostgreSQL Database
```bash
Start PostgreSQL server.

Create a new database:

CREATE DATABASE stock_etl;


Update database credentials in db_connection.py if needed:

DATABASE = "stock_etl"
USER = "postgres"
PASSWORD = "admin"
HOST = "localhost"
PORT = "5432"
```
### 5ï¸âƒ£ Test Database Connection
```bash
python test_db_connection.py


âœ… Should print: Connection successful!
```
## âš¡ Workflow

```bash
Step 1: Fetch & Clean Data

fetch_data.py â†’ Downloads historical stock data to data/raw/.

clean_data.py â†’ Cleans raw CSVs and saves to data/cleaned/.

check_cleaned.py â†’ Quick inspection of cleaned files.

Step 2: Load Data into Database

load_to_db.py â†’ Loads cleaned CSVs into PostgreSQL table stock_data.

Step 3: Run Analyses

Each analysis script saves outputs into its dedicated folder in outputs/.
```
## Script	Description
```bash
analyze_data.py	Average daily returns, closing price trends, and daily return histograms.
sector_trends.py	Closing price trends per sector.
sector_daily_returns.py	Daily return comparison per sector.
correlation_volatility.py	Correlation heatmap & rolling volatility of stocks.
top_movers_sector_summary.py	Top/bottom 5 movers & sector average returns.
moving_average_crossovers.py	Detects moving average crossover signals.
return_vs_volatility.py	Plots return vs volatility scatter for stocks.
volatility_vs_return.py	Another variant of risk-return scatter analysis.
trading_volume_analysis.py	Average trading volume per stock & sector.
drawdowns.py	Maximum drawdown per stock (risk measure).
cumulative_returns.py	Cumulative returns chart for all stocks.
seasonality_analysis.py	Monthly/quarterly return patterns.
export_summary_tables.py	Exports key summary tables (returns, volatility, etc.) to CSV.
```
## âœ… Notes

All outputs are stored in outputs/<script_name>/ folders.

Each script is independent and can be executed separately.

The project is modular; new analyses can be added by following the same structure.
